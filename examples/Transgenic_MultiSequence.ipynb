{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c30222",
   "metadata": {},
   "source": [
    "# TransGenic: Generating annotations over many sequences\n",
    "\n",
    "In this example, we will generate annotations for all the genic regions of chromosome 4 of the *Arabidopsis* genome. The file ```ATH_Chr4_gene.bed``` provides the location of each gene and will be used to construct a dataset of extracted DNA sequences from ```ATH_Chr4.fas```.\n",
    "\n",
    "### Constructing an inference dataset\n",
    "To pre-process the gene regions stored in the BED file into sequences useable by the model, we create a DuckDB database which collates the DNA sequences of each region with gene identidiers and chromosomal coordinates. The function below creates a DuckDB database called ```ath_chr4_predict.db```. Prediction datasets can be constructed from BED or GFF3 files. Note: GFF3 files should be sorted with AGAT or similar tool prior to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c89dcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-21 10:00:13,913] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgl/scratch1/jlomas/miniforge3/envs/transgenic/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/pgl/scratch1/jlomas/miniforge3/envs/transgenic/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ATH_Chr4_gene.bed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4128/4128 [00:16<00:00, 255.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets\n",
    "from transgenic.datasets.preprocess import genome2GSFDataset\n",
    "\n",
    "genome2GSFDataset(\n",
    "\t\"ATH_Chr4.fas\",\n",
    "\t\"ATH_Chr4_gene.bed\",\n",
    "\t\"ath_chr4_predict.db\",\n",
    "\tanoType=\"bed\",\n",
    "\tmode = \"predict\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea31ce98",
   "metadata": {},
   "source": [
    "Next, initialize a pytorch dataset and dataloader for use in the prediction loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f621a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transgenic.datasets.datasets import isoformDataHyena, hyena_collate_fn\n",
    "\n",
    "# Initialize a torch Dataset from the preprocessed database\n",
    "ds = isoformDataHyena(\n",
    "\t\"ath_chr4_predict.db\",\n",
    "\tmode = \"inference\"\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "dl = DataLoader(\n",
    "\tds,\n",
    "\tbatch_size=1,\n",
    "\tshuffle=False,\n",
    "\tnum_workers=0,\n",
    "\tpin_memory=True,\n",
    "\tcollate_fn=hyena_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4a51e",
   "metadata": {},
   "source": [
    "### *De novo* prediction and GFF output\n",
    "\n",
    "Loop through the sequences in the dataset to generate full annotations using a pretrained checkpoint. GSF model outputs are converted to GFF and written to an output file. For this example, we'll limit the output to 10 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08173b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/4127 [00:04<30:55,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transgenic.utils.gsf import gffString2GFF3\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n",
    "# Load the model\n",
    "model_name = \"jlomas/HyenaTransgenic-512L9A4-160M\"\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load the output tokenizer\n",
    "gffTokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Prediction loop \n",
    "for step, batch in enumerate(tqdm(dl)):\n",
    "\tif step == 10:\n",
    "\t\tbreak\n",
    "\t\n",
    "\tii, am = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "\t# Generate annotation with beam search\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\t\t\tinputs=ii, \n",
    "\t\t\t\t\tattention_mask=am, \n",
    "\t\t\t\t\tnum_return_sequences=1, \n",
    "\t\t\t\t\tmax_length=2048, \n",
    "\t\t\t\t\tnum_beams=2,\n",
    "\t\t\t\t\tdo_sample=True\n",
    "\t\t\t\t)\n",
    "\t# Decode the output to GSF\n",
    "\tpred = gffTokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0].replace(\"|</s>\", \"\").replace(\"</s>\", \"\").replace(\"<s>\", \"\")\n",
    "\t\n",
    "\t# Convert the GSF to GFF3\n",
    "\tgff = gffString2GFF3(pred, batch[4][0], batch[5][0], f\"GM={batch[3][0]}\")\n",
    "\t\n",
    "\t# Write the GFF3 output\n",
    "\twith open(\"ath_chr4_predict.gff\", \"a\") as f:\n",
    "\t\tfor line in gff:\n",
    "\t\t\tf.write(line + \"\\n\")\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071619f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr4\ttransgenic\tgene\t2875\t3270\t.\t+\t.\tID=efac4490-e16e-4153-ab41-ac535e8a0406;GM=AT4G00005\n",
      "Chr4\ttransgenic\tmRNA\t2875\t3270\t.\t+\t.\tID=efac4490-e16e-4153-ab41-ac535e8a0406.t1;Parent=efac4490-e16e-4153-ab41-ac535e8a0406;GM=AT4G00005\n",
      "Chr4\ttransgenic\tCDS\t2875\t3270\t.\t+\t0\tID=efac4490-e16e-4153-ab41-ac535e8a0406.t1.CDS1;Parent=efac4490-e16e-4153-ab41-ac535e8a0406.t1;GM=AT4G00005\n",
      "Chr4\ttransgenic\tgene\t2885\t10464\t.\t-\t.\tID=b0024f5a-da1d-47d6-9208-554ced090b63;GM=AT4G00020\n",
      "Chr4\ttransgenic\tmRNA\t2885\t10464\t.\t-\t.\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1;Parent=b0024f5a-da1d-47d6-9208-554ced090b63;GM=AT4G00020\n",
      "Chr4\ttransgenic\tCDS\t4075\t4438\t.\t-\t0\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1.CDS1;Parent=b0024f5a-da1d-47d6-9208-554ced090b63.t1;GM=AT4G00020\n",
      "Chr4\ttransgenic\tCDS\t4545\t4749\t.\t-\t0\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1.CDS2;Parent=b0024f5a-da1d-47d6-9208-554ced090b63.t1;GM=AT4G00020\n",
      "Chr4\ttransgenic\tCDS\t4839\t4901\t.\t-\t0\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1.CDS3;Parent=b0024f5a-da1d-47d6-9208-554ced090b63.t1;GM=AT4G00020\n",
      "Chr4\ttransgenic\tCDS\t4977\t5119\t.\t-\t0\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1.CDS4;Parent=b0024f5a-da1d-47d6-9208-554ced090b63.t1;GM=AT4G00020\n",
      "Chr4\ttransgenic\tCDS\t5212\t5268\t.\t-\t0\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1.CDS5;Parent=b0024f5a-da1d-47d6-9208-554ced090b63.t1;GM=AT4G00020\n"
     ]
    }
   ],
   "source": [
    "!head ath_chr4_predict.gff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81810b2a",
   "metadata": {},
   "source": [
    "### Prompt completion, predicting splice variants\n",
    "\n",
    "TransGenic can also be used to add splice variants to existing annotations. For this use case, we'll construct a dataset with GSF labels using the sorted reference GFF3 annotation. Then the features of the first transcript can be provided as input to the decoder to complete the annotation.\n",
    "\n",
    "Here, we'll create a GFF3 dataset with GSF labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec1d9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ATH_Chr4.sorted.gff3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 9880/81369 [00:01<00:13, 5234.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81369/81369 [00:15<00:00, 5117.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a new database, Dataset, and Dataloader\n",
    "genome2GSFDataset(\n",
    "\t\"ATH_Chr4.fas\",\n",
    "\t\"ATH_Chr4.sorted.gff3\",\n",
    "\t\"ath_chr4_train.db\",\n",
    "\tanoType=\"gff\",\n",
    "\tmode = \"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55083f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transgenic.datasets.datasets import isoformDataHyena, hyena_collate_fn\n",
    "\n",
    "# Initialize a torch Dataset from the preprocessed database\n",
    "ds_comp = isoformDataHyena(\n",
    "\t\"ath_chr4_train.db\",\n",
    "\tmode = \"train\"\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "dl_comp = DataLoader(\n",
    "\tds_comp,\n",
    "\tbatch_size=1,\n",
    "\tshuffle=False,\n",
    "\tnum_workers=0,\n",
    "\tpin_memory=True,\n",
    "\tcollate_fn=hyena_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bba1fc",
   "metadata": {},
   "source": [
    "Now, we can loop through the sequences while providing input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f8687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/4127 [00:01<09:12,  7.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transgenic.utils.gsf import gffString2GFF3\n",
    "\n",
    "# Prediction loop \n",
    "for step, batch in enumerate(tqdm(dl_comp)):\n",
    "\tif step == 10:\n",
    "\t\tbreak\n",
    "\t\n",
    "\tii, am, lab = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "\n",
    "\t# Get elements of first transcript to use as decoder input ids\n",
    "\tlabs = \",\".join([str(i) for i in lab.tolist()[0]])\n",
    "\tlast_element = labs.split(\",17,\")[1].split(\",21,\")[0].split(\",\")[-1]\n",
    "\ttry:\n",
    "\t\tlast_element_index = [f\",{last_element},\" in i for i in labs.split(\",17,\")[0].split(f\",21,\")].index(True)\n",
    "\texcept:\n",
    "\t\tlast_element_index = len(labs.split(\",17,\")[0].split(f\",21,\")) - 1\n",
    "\t\n",
    "\tdii = torch.tensor(list(map(int, \",21,\".join(labs.split(\",17,\")[0].split(f\",21,\")[0:last_element_index+1]).split(\",\")))).unsqueeze(0).to(device)\n",
    "\n",
    "\t# Generate annotation with beam search\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\t\t\tinputs=ii, \n",
    "\t\t\t\t\tattention_mask=am, \n",
    "\t\t\t\t\tnum_return_sequences=1, \n",
    "\t\t\t\t\tmax_length=2048, \n",
    "\t\t\t\t\tnum_beams=2,\n",
    "\t\t\t\t\tdo_sample=True,\n",
    "\t\t\t\t\tdecoder_input_ids = dii\n",
    "\t\t\t\t)\n",
    "\n",
    "\t# Decode the output to GSF\n",
    "\tpred = gffTokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0].replace(\"|</s>\", \"\").replace(\"</s>\", \"\").replace(\"<s>\", \"\")\n",
    "\t\n",
    "\t# Convert the GSF to GFF3\n",
    "\tgff = gffString2GFF3(pred, batch[4][0], batch[5][0], f\"GM={batch[3][0]}\")\n",
    "\t\n",
    "\t# Write the GFF3 output\n",
    "\twith open(\"ath_chr4_completion.gff\", \"a\") as f:\n",
    "\t\tfor line in gff:\n",
    "\t\t\tf.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b527be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chr4\ttransgenic\tgene\t2875\t3270\t.\t+\t.\tID=efac4490-e16e-4153-ab41-ac535e8a0406;GM=AT4G00005\n",
      "Chr4\ttransgenic\tmRNA\t2875\t3270\t.\t+\t.\tID=efac4490-e16e-4153-ab41-ac535e8a0406.t1;Parent=efac4490-e16e-4153-ab41-ac535e8a0406;GM=AT4G00005\n",
      "Chr4\ttransgenic\tCDS\t2875\t3270\t.\t+\t0\tID=efac4490-e16e-4153-ab41-ac535e8a0406.t1.CDS1;Parent=efac4490-e16e-4153-ab41-ac535e8a0406.t1;GM=AT4G00005\n",
      "Chr4\ttransgenic\tgene\t2885\t10464\t.\t-\t.\tID=b0024f5a-da1d-47d6-9208-554ced090b63;GM=AT4G00020\n",
      "Chr4\ttransgenic\tmRNA\t2885\t10464\t.\t-\t.\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1;Parent=b0024f5a-da1d-47d6-9208-554ced090b63;GM=AT4G00020\n",
      "Chr4\ttransgenic\tCDS\t4075\t4438\t.\t-\t0\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1.CDS1;Parent=b0024f5a-da1d-47d6-9208-554ced090b63.t1;GM=AT4G00020\n",
      "Chr4\ttransgenic\tCDS\t4545\t4749\t.\t-\t0\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1.CDS2;Parent=b0024f5a-da1d-47d6-9208-554ced090b63.t1;GM=AT4G00020\n",
      "Chr4\ttransgenic\tCDS\t4839\t4901\t.\t-\t0\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1.CDS3;Parent=b0024f5a-da1d-47d6-9208-554ced090b63.t1;GM=AT4G00020\n",
      "Chr4\ttransgenic\tCDS\t4977\t5119\t.\t-\t0\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1.CDS4;Parent=b0024f5a-da1d-47d6-9208-554ced090b63.t1;GM=AT4G00020\n",
      "Chr4\ttransgenic\tCDS\t5212\t5268\t.\t-\t0\tID=b0024f5a-da1d-47d6-9208-554ced090b63.t1.CDS5;Parent=b0024f5a-da1d-47d6-9208-554ced090b63.t1;GM=AT4G00020\n"
     ]
    }
   ],
   "source": [
    "!head ath_chr4_predict.gff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transgenic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
